{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 66749,
     "status": "ok",
     "timestamp": 1583133169331,
     "user": {
      "displayName": "Holden Hu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh3iq2ZGde3yRzXA9BJJWA-ixxE3rCZ9YkCNIPc=s64",
      "userId": "15821964663637171132"
     },
     "user_tz": -480
    },
    "id": "Hg8Dcu3E-iV4",
    "outputId": "992dba8a-8d54-4a9f-ecab-8e11e66105fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'bert'...\n",
      "remote: Enumerating objects: 336, done.\u001b[K\n",
      "remote: Total 336 (delta 0), reused 0 (delta 0), pack-reused 336\u001b[K\n",
      "Receiving objects: 100% (336/336), 297.11 KiB | 95.00 KiB/s, done.\n",
      "Resolving deltas: 100% (183/183), done.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf bert\n",
    "!git clone https://github.com/google-research/bert bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cEegB0MWFz9e"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "import copy \n",
    "\n",
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "  When running eval/predict on the TPU, we need to pad the number of examples\n",
    "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "  size. The alternative is to drop the last batch, which is bad because it means\n",
    "  the entire output data won't be generated.\n",
    "  We use this class instead of `None` because treating `None` as padding\n",
    "  battches could cause silent errors.\n",
    "  \"\"\"\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, labels=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.labels = labels\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    bert_module =  hub.Module(bert_path)\n",
    "#     bert_module =  hub.load(bert_path)\n",
    "\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "#     tokenization_info = bert_module.signatures[\"tokenization_info\"]\n",
    "\n",
    "    vocab_file, do_lower_case = sess.run(\n",
    "        [\n",
    "            tokenization_info[\"vocab_file\"],\n",
    "            tokenization_info[\"do_lower_case\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        input_ids = [0] * max_seq_length\n",
    "        input_mask = [0] * max_seq_length\n",
    "        segment_ids = [0] * max_seq_length\n",
    "        labels = [tags[\"-PAD-\"]] * max_seq_length\n",
    "        return input_ids, input_mask, segment_ids, labels\n",
    "\n",
    "    new_labels = copy.deepcopy(example.labels)\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    \n",
    "    for idx, t in enumerate(tokens_a):\n",
    "        try:\n",
    "            dummy = new_labels[idx]\n",
    "        except IndexError as e:\n",
    "            new_labels.insert(idx, new_labels[idx-1])\n",
    "        if t[:2] == \"##\":\n",
    "            new_labels.insert(idx, new_labels[idx-1])        \n",
    "        \n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
    "        new_labels = new_labels[0 : (max_seq_length - 2)]\n",
    "\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    labels = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    labels.append(tag2idx[\"-PAD-\"])\n",
    "    for i, token in enumerate(tokens_a):\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "        labels.append(new_labels[i])\n",
    "    labels.append(tag2idx[\"-PAD-\"])\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    while len(labels) < max_seq_length:\n",
    "        labels.append(tag2idx[\"-PAD-\"])\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "    assert len(labels) == max_seq_length\n",
    "    \n",
    "    return input_ids, input_mask, segment_ids, labels\n",
    "\n",
    "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
    "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "\n",
    "    input_ids, input_masks, segment_ids, labels_arr, shapetags_arr = [], [], [], [], []\n",
    "    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n",
    "        input_id, input_mask, segment_id, labels = convert_single_example(\n",
    "            tokenizer, example, max_seq_length\n",
    "        )\n",
    "        input_ids.append(input_id)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "        labels_arr.append(labels)\n",
    "    return (\n",
    "        np.array(input_ids),\n",
    "        np.array(input_masks),\n",
    "        np.array(segment_ids),\n",
    "        np.array([to_categorical(i, num_classes=n_tags) for i in labels_arr]),\n",
    "    )\n",
    "\n",
    "def convert_text_to_examples(texts, labels_arr):\n",
    "    \"\"\"Create InputExamples\"\"\"\n",
    "    InputExamples = []\n",
    "    for text, labels in zip(texts, labels_arr):\n",
    "        InputExamples.append(\n",
    "            InputExample(guid=None, text_a=\" \".join(text), text_b=None,\n",
    "                         labels=labels)\n",
    "        )\n",
    "    return InputExamples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2507,
     "status": "ok",
     "timestamp": 1583133732928,
     "user": {
      "displayName": "Holden Hu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh3iq2ZGde3yRzXA9BJJWA-ixxE3rCZ9YkCNIPc=s64",
      "userId": "15821964663637171132"
     },
     "user_tz": -480
    },
    "id": "2Bwjkf7qGXkt",
    "outputId": "b577f35f-bed5-46fc-b139-3b1a4add6aa6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-02 15:59:17.228 INFO in 'deeppavlov.core.data.utils'['utils'] at line 80: Downloading from http://files.deeppavlov.ai/deeppavlov_data/conll2003_v2.tar.gz to data/conll2003_v2.tar.gz\n",
      "100%|██████████| 957k/957k [00:03<00:00, 274kB/s] \n",
      "2020-03-02 15:59:20.724 INFO in 'deeppavlov.core.data.utils'['utils'] at line 237: Extracting data/conll2003_v2.tar.gz archive into data\n"
     ]
    }
   ],
   "source": [
    "import deeppavlov\n",
    "from deeppavlov.core.data.utils import download_decompress\n",
    "download_decompress('http://files.deeppavlov.ai/deeppavlov_data/conll2003_v2.tar.gz', 'data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RR-E7F2uGeGY"
   },
   "outputs": [],
   "source": [
    "from deeppavlov.dataset_readers.conll2003_reader import Conll2003DatasetReader\n",
    "dataset = Conll2003DatasetReader().read('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vH7qqewpGhbI"
   },
   "outputs": [],
   "source": [
    "train_words, train_tags = [], []\n",
    "for tpl in dataset['train']:\n",
    "    train_words.append(tpl[0])\n",
    "    train_tags.append(tpl[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bCcw4iEYGkyo"
   },
   "outputs": [],
   "source": [
    "tags = set([])\n",
    "\n",
    "for ts in train_tags:\n",
    "  for i in ts:\n",
    "    tags.add(i)\n",
    "tags = list(tags)\n",
    "tag2idx = {t: i+1 for i, t in enumerate(list(tags))}\n",
    "tag2idx[\"-PAD-\"] = 0 # for the mask zero\n",
    "n_tags = len(tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rGElsrxIGoHS"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# import tensorflow.compat.v1 as tf\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from bert.tokenization import FullTokenizer\n",
    "from tqdm import tqdm_notebook\n",
    "from keras import backend as K\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "# sess = tf.compat.v1.Session()\n",
    "\n",
    "# Params for bert model and tokenization\n",
    "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "max_seq_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hIMvMYExGp4A"
   },
   "outputs": [],
   "source": [
    "train_tag_ids = [list(map(lambda x: tag2idx[x], sample)) for sample in train_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100,
     "referenced_widgets": [
      "e4642bc0058e4128a99b5c63eb3c2355",
      "fe3ef33621714f5387389bf2703225de",
      "e0e890da439c486d8e7bf319987afa35",
      "934fbd06bcec4ab19cadddc868fa9986",
      "640229b92a4743ee859c3311e00ab249",
      "1b2650603a3941118981e69d4e11d532",
      "187bdb53efd746ee81d4b3b8b9c4becd",
      "5da65e6e13f3426db1722a0c60dfc13f"
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13312,
     "status": "ok",
     "timestamp": 1583133769169,
     "user": {
      "displayName": "Holden Hu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh3iq2ZGde3yRzXA9BJJWA-ixxE3rCZ9YkCNIPc=s64",
      "userId": "15821964663637171132"
     },
     "user_tz": -480
    },
    "id": "7j5ooS9uGsxw",
    "outputId": "063e30ed-6cff-4e54-faef-4b202f5a5320"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization_info:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'signatures'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-749125795b13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Instantiate tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_tokenizer_from_hub_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Convert data to InputExample format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_text_to_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tag_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-715c3a4aef84>\u001b[0m in \u001b[0;36mcreate_tokenizer_from_hub_module\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#     tokenization_info = bert_module.signatures[\"tokenization_info\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenization_info:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenization_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     vocab_file, do_lower_case = sess.run(\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'signatures'"
     ]
    }
   ],
   "source": [
    "# Instantiate tokenizer\n",
    "tokenizer = create_tokenizer_from_hub_module()\n",
    "\n",
    "# Convert data to InputExample format\n",
    "train_examples = convert_text_to_examples(train_words, train_tag_ids)\n",
    "\n",
    "# Convert to features\n",
    "(train_input_ids, train_input_masks, train_segment_ids, train_tag_ids \n",
    ") = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_k4uW8EnG6wY"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "\n",
    "class BertLayer(Layer):\n",
    "    def __init__(self, n_fine_tune_layers=10, mask_zero=False, trainable=True, **kwargs):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = trainable\n",
    "        self.output_size = 768\n",
    "        self.mask_zero=mask_zero\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            bert_path,\n",
    "            trainable=self.trainable,\n",
    "            name=\"{}_module\".format(self.name)\n",
    "        )\n",
    "        \n",
    "        # TRAINABLE PARAMS: TODO: Test that if have time\n",
    "#         trainable_vars = self.bert.variables\n",
    "        \n",
    "        # Remove unused layers\n",
    "#         trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "        \n",
    "        # Select how many layers to fine tune\n",
    "#         trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n",
    "#         \n",
    "        # Add to trainable weights\n",
    "#         for var in trainable_vars:\n",
    "#             print(var)\n",
    "#             self._trainable_weights.append(var)\n",
    "        \n",
    "         # Remove unused layers and set trainable parameters\n",
    "        self.trainable_weights += [var for var in self.bert.variables\n",
    "                                   if not \"/cls/\" in var.name and not \"/pooler/\" in var.name][-self.n_fine_tune_layers :]\n",
    "\n",
    "        # Add non-trainable weights\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self.trainable_weights:\n",
    "                self.non_trainable_weights.append(var)\n",
    "                \n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(input_ids=input_ids, input_mask=input_mask,\n",
    "                           segment_ids=segment_ids)\n",
    "        result = self.bert(inputs=bert_inputs, signature=\"tokens\",\n",
    "                           as_dict=True)[\"sequence_output\"]\n",
    "        result = K.reshape(result, (-1,inputs[0].shape[1],768))\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, input_shape[0][1], self.output_size)\n",
    "      \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "      input_ids, input_mask, segment_ids = inputs\n",
    "      if not self.mask_zero:\n",
    "          return None\n",
    "      return K.not_equal(input_ids, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cVLxi8WMHG4M"
   },
   "outputs": [],
   "source": [
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_accuracy, crf_viterbi_accuracy\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import Embedding, Dense, Bidirectional, Dropout, LSTM, TimeDistributed\n",
    "\n",
    "# Build model\n",
    "def build_model(max_seq_length):  \n",
    "    # Bert Embeddings\n",
    "    in_id = Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "    in_mask = Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "    in_segment = Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    bert_output = BertLayer(n_fine_tune_layers=10, mask_zero=True, trainable=True)(bert_inputs)\n",
    "    \n",
    "    lstm = Bidirectional(LSTM(units=128, return_sequences=True))(bert_output)\n",
    "    drop = Dropout(0.4)(lstm)\n",
    "    dense = TimeDistributed(Dense(128, activation=\"relu\"))(drop)\n",
    "    crf = CRF(n_tags)\n",
    "    out = crf(dense)\n",
    "    model = Model(inputs=bert_inputs, outputs=out)\n",
    "    model.compile(loss=crf.loss_function, optimizer='adam', metrics=[crf.accuracy])\n",
    "    model.summary()\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "  \n",
    "def initialize_vars(sess):\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    K.get_session().run(tf.global_variables_initializer())\n",
    "    K.get_session().run(tf.tables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5585,
     "status": "error",
     "timestamp": 1583134043869,
     "user": {
      "displayName": "Holden Hu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh3iq2ZGde3yRzXA9BJJWA-ixxE3rCZ9YkCNIPc=s64",
      "userId": "15821964663637171132"
     },
     "user_tz": -480
    },
    "id": "g89XEfn-HITH",
    "outputId": "1ec71a86-17c1-42dc-fabb-8afacd583ee3"
   },
   "outputs": [],
   "source": [
    "model = build_model(max_seq_length)\n",
    "\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NMKZobAnJRnq"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids], \n",
    "    train_tag_ids,\n",
    "    validation_split=0.2,\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNOX438YJKXNEVy2Rm9kZIs",
   "collapsed_sections": [],
   "name": "BERT-CoNLL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "187bdb53efd746ee81d4b3b8b9c4becd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1b2650603a3941118981e69d4e11d532": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5da65e6e13f3426db1722a0c60dfc13f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "640229b92a4743ee859c3311e00ab249": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "934fbd06bcec4ab19cadddc868fa9986": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5da65e6e13f3426db1722a0c60dfc13f",
      "placeholder": "​",
      "style": "IPY_MODEL_187bdb53efd746ee81d4b3b8b9c4becd",
      "value": " 14041/14041 [00:26&lt;00:00, 538.72it/s]"
     }
    },
    "e0e890da439c486d8e7bf319987afa35": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Converting examples to features: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b2650603a3941118981e69d4e11d532",
      "max": 14041,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_640229b92a4743ee859c3311e00ab249",
      "value": 14041
     }
    },
    "e4642bc0058e4128a99b5c63eb3c2355": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e0e890da439c486d8e7bf319987afa35",
       "IPY_MODEL_934fbd06bcec4ab19cadddc868fa9986"
      ],
      "layout": "IPY_MODEL_fe3ef33621714f5387389bf2703225de"
     }
    },
    "fe3ef33621714f5387389bf2703225de": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
